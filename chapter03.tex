\section{Linear Methods for Regression}

%3.1
\begin{sol}
Let $\hat{\beta}$ be the parameter estimation for the bigger model and $\tilde{\beta}$ be the one for the smaller model. We first have
\begin{align*}
(\mathrm{RSS}_0-\mathrm{RSS}_1)/(p_1-p_0) = & \|y-X\tilde{\beta}\|^2 - \|y-X\hat{\beta}\|^2 \\
=& (y^\mathrm{T}y-2\tilde{\beta}^\mathrm{T}X^\mathrm{T}y+\tilde{\beta}^\mathrm{T}X^\mathrm{T}X\tilde{\beta}) - (y^\mathrm{T}y-2\hat{\beta}^\mathrm{T}X^\mathrm{T}y+\hat{\beta}^\mathrm{T}X^\mathrm{T}X\hat{\beta}) \\
=& 2(\hat{\beta}-\tilde{\beta})^\mathrm{T}X^\mathrm{T}(y-X\hat{\beta}) + (\hat{\beta}-\tilde{\beta})^\mathrm{T}X^\mathrm{T}X (\hat{\beta}-\tilde{\beta}) \\
=& 2(\hat{\beta}-\tilde{\beta})^\mathrm{T}(X^\mathrm{T}y-X^\mathrm{T}X(X^\mathrm{T}X)^{-1}X^\mathrm{T}y) + (\hat{\beta}-\tilde{\beta})^\mathrm{T}X^\mathrm{T}X (\hat{\beta}-\tilde{\beta}) \\
=& (\hat{\beta}-\tilde{\beta})^\mathrm{T}X^\mathrm{T}X (\hat{\beta}-\tilde{\beta})\\
=& \|X (\hat{\beta}-\tilde{\beta})\|^2
\end{align*}
Note that $\hat{\beta}$ and $\tilde{\beta}$ satisfy
\[
\hat{\beta}=\argmin_\beta\|y-X\beta\|^2
\]
and
\begin{align*}
\tilde{\beta}=&\argmin_\beta\|y-X\beta\|^2\\
\st &\beta_j=0
\end{align*}
Denote $e_j$ as the vector with the $j$-th element being 1 and others being 0. The optimality conditions are
\[
X^\mathrm{T}(y-X\hat{\beta})=0
\]
and
\[
\begin{cases}
& X^\mathrm{T}(y-X\tilde{\beta})-\lambda e_j=0\\
& \tilde{\beta}_j=0
\end{cases}
\]
which gives
\[
X^\mathrm{T}X (\hat{\beta}-\tilde{\beta}) = \lambda e_j \Longrightarrow \|X (\hat{\beta}-\tilde{\beta})\|^2 = \lambda^2 (X^\mathrm{T}X)^{-1}_{jj} = \lambda^2 (X^\mathrm{T}X)^{-1}_{jj}
\]
To determine the value of the Lagrange multiplier $\lambda$, one only needs to notice that
\begin{align*}
&\hat{\beta}-\tilde{\beta} = \lambda (X^\mathrm{T}X)^{-1} e_j = \lambda (X^\mathrm{T}X)^{-1}_j \\
\Longrightarrow & \hat{\beta}_j = \lambda (X^\mathrm{T}X)^{-1}_{jj} \\
\Longrightarrow & \lambda = \frac{\hat{\beta}_j}{(X^\mathrm{T}X)^{-1}_{jj}}
\end{align*}
where $(X^\mathrm{T}X)^{-1}_j$ is the $j$-th column of $(X^\mathrm{T}X)^{-1}$. If define $v_j\triangleq(X^\mathrm{T}X)^{-1}_{jj}$ and recall $\mathrm{RSS}_1/(N-p_1-1)=\hat{\sigma}^2$, we finally have
\[
F = \left(\frac{\hat{\beta}_j}{v_j}\right)^2 \frac{v_j}{\hat{\sigma}^2} = \frac{\hat{\beta}_j^2}{\hat{\sigma}^2v_j}
\]
\textbf{Update}: By the relationship between $t$-distribution and $F$-distribution
\begin{align*}
t_\nu=& \frac{Z}{\sqrt{\chi_\nu^2/\nu}}\\
=& \frac{\sqrt{\chi_1^2/1}}{\sqrt{\chi_\nu^2/\nu}}\\
=& \sqrt{F_{1,\nu}}
\end{align*}
one can finish the proof immediately.
\end{sol}

%3.2
\begin{sol}
%\begin{enumerate}
%\item Pointwise confidence bands
%\item Simultaneous confidence bands
%\end{enumerate}
\end{sol}

%3.3
\begin{sol}
\begin{enumerate}
\item
\label{3.3-1}
Let $\theta=a^\mathrm{T}\beta$, $\hat{\theta}=a^\mathrm{T}\hat{\beta}=b^\mathrm{T}y$ and $\tilde{\theta}=c^\mathrm{T}y$, where $\hat{\beta}=(X^{\mathrm{T}}X)^{-1}X^{\mathrm{T}}y$, $b=X(X^{\mathrm{T}}X)^{-1}a$ and $y\sim\mathcal{N}(X\beta,\sigma^2I)$. From the unbiasedness of $\tilde{\theta}$, we know $\E[\tilde{\theta}]=c^{\mathrm{T}}X\beta=\E[\hat{\theta}]=a^\mathrm{T}\beta$. Expanding $\var[\hat{\theta}]$ and $\var[\tilde{\theta}]$ we will have
\begin{align*}
\var[\hat{\theta}] =& b^\mathrm{T}(\sigma^2 I)b\\
=& \sigma^2 a^\mathrm{T}(X^{\mathrm{T}}X)^{-1}X^\mathrm{T}X(X^{\mathrm{T}}X)^{-1}a\\
=& \sigma^2 a^\mathrm{T}(X^{\mathrm{T}}X)^{-1}a\\
\var[\tilde{\theta}] =& c^\mathrm{T}(\sigma^2 I)c\\
=& \sigma^2 c^\mathrm{T}c
\end{align*}
Now consider
\begin{align*}
\min_c \ & c^\mathrm{T}c\\
\st & c^{\mathrm{T}}X\beta=a^\mathrm{T}\beta
\end{align*}
The Lagrangian $L=c^\mathrm{T}c+\lambda(c^{\mathrm{T}}X\beta-a^\mathrm{T}\beta)$ gives the stationary condition:
\begin{align*}
\frac{\partial L}{\partial c}=& 2c+\lambda X\beta=0\\
\frac{\partial L}{\partial \lambda}=& c^{\mathrm{T}}X\beta-a^\mathrm{T}\beta=0
\end{align*}
Canceling the $\lambda$ gives
\[
c=\frac{a^\mathrm{T}\beta}{\beta^{\mathrm{T}}X^{\mathrm{T}}X\beta}\cdot X\beta
\]
So
\begin{align*}
& c^\mathrm{T}c\ge \frac{\left(a^\mathrm{T}\beta\right)^2}{\beta^{\mathrm{T}}X^{\mathrm{T}}X\beta}=\frac{\beta^{\mathrm{T}}aa^{\mathrm{T}}\beta}{\beta^{\mathrm{T}}X^{\mathrm{T}}X\beta}\\
\Longrightarrow & c^\mathrm{T}c\ge \max_\beta \frac{\beta^{\mathrm{T}}aa^{\mathrm{T}}\beta}{\beta^{\mathrm{T}}X^{\mathrm{T}}X\beta}
\end{align*}
The final optimization problem is equivalent to
\begin{align*}
\max_\beta \ & \beta^{\mathrm{T}}aa^{\mathrm{T}}\beta\\
\st & \beta^{\mathrm{T}}X^{\mathrm{T}}X\beta=1
\end{align*}
Again by Lagrange multiplier, we can see that the maximum is the largest eigenvalue, denoted as $\mu$, of $A=aa^{\mathrm{T}}(X^{\mathrm{T}}X)^{-1}$. But since $\mathrm{rank}(A)=1$, so $\mu=\tr(A)=a^{\mathrm{T}}(X^{\mathrm{T}}X)^{-1}a$.
\item We similarly let $\tilde{\beta}=Cy\in\mathbb{R}^{p+1}$, then $\hat{V}=\sigma^2(X^\mathrm{T}X)^{-1}$, $\tilde{V}=\sigma^2CC^{\mathrm{T}}$ and $\E[\tilde{\beta}]=CX\beta=\beta$. From \ref{3.3-1}, $\forall a\in\mathbb{R}^{p+1}$, given $a^{\mathrm{T}}CX\beta=a^{\mathrm{T}}\beta$, we have
\begin{align*}
& a^{\mathrm{T}}CC^{\mathrm{T}}a\ge a^{\mathrm{T}}(X^\mathrm{T}X)^{-1}a\\
\Longrightarrow & a^{\mathrm{T}}\left(CC^{\mathrm{T}}-(X^\mathrm{T}X)^{-1}\right)a\ge 0\\
\Longrightarrow & CC^{\mathrm{T}} - \left(X^\mathrm{T}X\right)^{-1} \succeq 0\\
\Longrightarrow & CC^{\mathrm{T}}\succeq \left(X^\mathrm{T}X\right)^{-1}
\end{align*}
\end{enumerate}
\end{sol}

%3.4
\begin{sol}
From $X=QR$ one can get $\hat{\beta}=(X^\mathrm{T}X)^{-1}X^\mathrm{T}y=R^{-1}Q^\mathrm{T}y\Longrightarrow R\hat{\beta}=Q^\mathrm{T}y$. Further we denote
\begin{align*}
X =& (x_1,\cdots,x_{p+1})\\
Q =& (e_1,\cdots,e_{p+1})=(\frac{z_1}{\|z_1\|},\cdots,\frac{z_{p+1}}{\|z_{p+1}\|})\\
r_{ij} =& e_i^\mathrm{T}x_j
\end{align*}
Then we can write the equations involving individual $\hat{\beta}_i$'s in a ``bottom-up'' way
\begin{align*}
(e_{p+1}^\mathrm{T}x_{p+1})\hat{\beta}_{p+1} &= e_{p+1}^\mathrm{T}y\\
(e_{p}^\mathrm{T}x_{p})\hat{\beta}_{p} + (e_{p}^\mathrm{T}x_{p+1})\hat{\beta}_{p+1} &= e_{p}^\mathrm{T}y\\
&\vdotswithin{=}\\
(e_{1}^\mathrm{T}x_{1})\hat{\beta}_{1} + \cdots + (e_{1}^\mathrm{T}x_{p+1})\hat{\beta}_{p+1} &= e_{1}^\mathrm{T}y
\end{align*}
This can be solved sequentially:
\begin{align*}
\hat{\beta}_{p+1} &= \frac{e_{p+1}^\mathrm{T}y}{e_{p+1}^\mathrm{T}x_{p+1}}\\
\hat{\beta}_{p} &= \frac{e_{p}^\mathrm{T}y}{e_{p}^\mathrm{T}x_{p}}-\frac{e_{p}^\mathrm{T}x_{p+1}}{e_{p}^\mathrm{T}x_{p}}\hat{\beta}_{p+1}\\
&\vdotswithin{=}\\
\hat{\beta}_{1} &= \frac{e_{1}^\mathrm{T}y}{e_{1}^\mathrm{T}x_{1}}- \frac{e_{1}^\mathrm{T}x_{2}}{e_{1}^\mathrm{T}x_{1}}\hat{\beta}_{2} - \cdots - \frac{e_{1}^\mathrm{T}x_{p+1}}{e_{1}^\mathrm{T}x_{1}}\hat{\beta}_{p+1}
\end{align*}
\end{sol}

%3.5
\begin{sol}
\begin{enumerate}
\item Ridge regression. From
\begin{align*}
\mathcal{L} =& \sum_{i=1}^{N}\left(y_i-\beta_0-\sum_{j=1}^{p}x_{ij}\beta_j\right)^2+\lambda\sum_{j=1}^{p}\beta_j^2\\
=& \sum_{i=1}^{N}\left[y_i-\left(\beta_0+\sum_{j=1}^{p}\bar{x}_j\beta_j\right)-\sum_{j=1}^{p}\left(x_{ij}-\bar{x}_j\right)\beta_j\right]^2+\lambda\sum_{j=1}^{p}\beta_j^2
\end{align*}
we can re-parameterize $\beta$ accordingly:
\[
\begin{cases}
\beta_0^c=\beta_0+\sum_{j=1}^{p}\bar{x}_j\beta_j\\
\beta_j^c=\beta_j\quad(1\le j\le p)
\end{cases}
\]
By setting $\partial \mathcal{L}/\partial \beta^c_0=0$, we have
\begin{align*}
& \sum_{i=1}^{N}\left[y_i-\beta_0^c-\sum_{j=1}^{p}\left(x_{ij}-\bar{x}_j\right)\beta^c_j\right]=0\\
\Longrightarrow & \beta_0^c=\frac{1}{N}\sum_{i=1}^{N}y_i-\frac{1}{N}\sum_{j=1}^{p}\sum_{i=1}^{N}\left(x_{ij}-\bar{x}_j\right)\beta^c_j=\frac{1}{N}\sum_{i=1}^{N}y_i
\end{align*}
Then one can follow the routine of ridge regression to get other $\beta_j^c$'s with the centered design matrix.
\item Lasso. The regularization term does not affect the derivation of $\beta_0^c$, so we have the same $\beta_0^c$, with other $\beta_j^c$'s obtained from any Lasso solver.
\end{enumerate}
\end{sol}

%3.6
\begin{sol}
From
\begin{align*}
P(\beta\vert \mathbf{y}) \propto & P(\mathbf{y}\vert\beta)P(\beta)\\
\propto& \exp\left[-\frac{1}{2\sigma^2}\left(\|\mathbf{y}-\mathbf{X}\beta\|^2+\frac{\sigma^2}{\tau}\|\beta\|^2\right)\right]
\end{align*}
we immediately know that $\lambda=\sigma^2/\tau$ and ridge regression estimate is the mode of the posterior distribution. To prove that it's also the mean, we let $\hat{\beta}$ be the ridge regression estimate:
\[
\hat{\beta}=\left(\mathbf{X}^\mathrm{T}\mathbf{X}+\frac{\sigma^2}{\tau}I\right)^{-1}\mathbf{X}^\mathrm{T}\mathbf{y}
\]
then
\begin{align*}
& \|\mathbf{y}-\mathbf{X}\beta\|^2+\frac{\sigma^2}{\tau}\|\beta\|^2 \\
=& \beta^\mathrm{T}\left(\mathbf{X}^\mathrm{T}\mathbf{X}+\frac{\sigma^2}{\tau}I\right)\beta - 2\mathbf{y}^\mathrm{T}\mathbf{X}\beta + \mathbf{y}^\mathrm{T}\mathbf{y}\\
=& (\beta-\hat{\beta})^\mathrm{T}\left(\mathbf{X}^\mathrm{T}\mathbf{X}+\frac{\sigma^2}{\tau}I\right)(\beta-\hat{\beta})-\hat{\beta}^\mathrm{T}\left(\mathbf{X}^\mathrm{T}\mathbf{X}+\frac{\sigma^2}{\tau}I\right)\hat{\beta}+\|\mathbf{y}\|^2
\end{align*}
So $\hat{\beta}$ also maximizes the likelihood of the posterior distribution.
\end{sol}
