\section{Linear Methods for Regression}

%3.1
\begin{sol}
\label{3.1}
Let $\hat{\beta}$ be the parameter estimation for the bigger model and $\tilde{\beta}$ be the one for the smaller model. We first have
\begin{align*}
(\mathrm{RSS}_0-\mathrm{RSS}_1)/(p_1-p_0) = & \|y-X\tilde{\beta}\|^2 - \|y-X\hat{\beta}\|^2 \\
=& (y^\mathrm{T}y-2\tilde{\beta}^\mathrm{T}X^\mathrm{T}y+\tilde{\beta}^\mathrm{T}X^\mathrm{T}X\tilde{\beta}) - (y^\mathrm{T}y-2\hat{\beta}^\mathrm{T}X^\mathrm{T}y+\hat{\beta}^\mathrm{T}X^\mathrm{T}X\hat{\beta}) \\
=& 2(\hat{\beta}-\tilde{\beta})^\mathrm{T}X^\mathrm{T}(y-X\hat{\beta}) + (\hat{\beta}-\tilde{\beta})^\mathrm{T}X^\mathrm{T}X (\hat{\beta}-\tilde{\beta}) \\
=& 2(\hat{\beta}-\tilde{\beta})^\mathrm{T}(X^\mathrm{T}y-X^\mathrm{T}X(X^\mathrm{T}X)^{-1}X^\mathrm{T}y) + (\hat{\beta}-\tilde{\beta})^\mathrm{T}X^\mathrm{T}X (\hat{\beta}-\tilde{\beta}) \\
=& (\hat{\beta}-\tilde{\beta})^\mathrm{T}X^\mathrm{T}X (\hat{\beta}-\tilde{\beta})\\
=& \|X (\hat{\beta}-\tilde{\beta})\|^2
\end{align*}
Note that $\hat{\beta}$ and $\tilde{\beta}$ satisfy
\[
\hat{\beta}=\argmin_\beta\|y-X\beta\|^2
\]
and
\begin{align*}
\tilde{\beta}=&\argmin_\beta\|y-X\beta\|^2\\
\st &\beta_j=0
\end{align*}
Denote $e_j$ as the vector with the $j$-th element being 1 and others being 0. The optimality conditions are
\[
X^\mathrm{T}(y-X\hat{\beta})=0
\]
and
\[
\begin{cases}
& X^\mathrm{T}(y-X\tilde{\beta})-\lambda e_j=0\\
& \tilde{\beta}_j=0
\end{cases}
\]
which gives
\[
X^\mathrm{T}X (\hat{\beta}-\tilde{\beta}) = \lambda e_j \Longrightarrow \|X (\hat{\beta}-\tilde{\beta})\|^2 = \lambda^2 (X^\mathrm{T}X)^{-1}_{jj} = \lambda^2 (X^\mathrm{T}X)^{-1}_{jj}
\]
To determine the value of the Lagrange multiplier $\lambda$, one only needs to notice that
\begin{align*}
&\hat{\beta}-\tilde{\beta} = \lambda (X^\mathrm{T}X)^{-1} e_j = \lambda (X^\mathrm{T}X)^{-1}_j \\
\Longrightarrow & \hat{\beta}_j = \lambda (X^\mathrm{T}X)^{-1}_{jj} \\
\Longrightarrow & \lambda = \frac{\hat{\beta}_j}{(X^\mathrm{T}X)^{-1}_{jj}}
\end{align*}
where $(X^\mathrm{T}X)^{-1}_j$ is the $j$-th column of $(X^\mathrm{T}X)^{-1}$. If define $v_j\triangleq(X^\mathrm{T}X)^{-1}_{jj}$ and recall $\mathrm{RSS}_1/(N-p_1-1)=\hat{\sigma}^2$, we finally have
\[
F = \left(\frac{\hat{\beta}_j}{v_j}\right)^2 \frac{v_j}{\hat{\sigma}^2} = \frac{\hat{\beta}_j^2}{\hat{\sigma}^2v_j}
\]
\textbf{Update}: By the relationship between $t$-distribution and $F$-distribution
\begin{align*}
t_\nu=& \frac{Z}{\sqrt{\chi_\nu^2/\nu}}\\
=& \frac{\sqrt{\chi_1^2/1}}{\sqrt{\chi_\nu^2/\nu}}\\
=& \sqrt{F_{1,\nu}}
\end{align*}
one can finish the proof immediately.
\end{sol}

%3.2
\begin{sol}
The main difference happens in the order between taking projection and calculating confidence region.
\begin{enumerate}
\item This corresponds to first projecting $\beta$ onto $\alpha$ then calculating the confidence interval. We have
\begin{align*}
&\hat{\beta}\sim\mathcal{N}(\beta, \sigma^2(X^TX)^{-1})\\
\Longrightarrow & \alpha^\mathrm{T}\hat{\beta}\sim\mathcal{N}(\alpha^\mathrm{T}\beta, \sigma^2\alpha^\mathrm{T}(X^TX)^{-1}\alpha)\\
\Longrightarrow & \frac{\alpha^\mathrm{T}\hat{\beta} - \alpha^\mathrm{T}\beta}{\hat{\sigma}\sqrt{\alpha^\mathrm{T}(X^TX)^{-1}\alpha}} \sim t_{N-p-1}
\end{align*}
So the 95\% confidence band is
\[
\left[\alpha^\mathrm{T}\hat{\beta}-\hat{\sigma}\sqrt{\alpha^\mathrm{T}(X^TX)^{-1}\alpha}\cdot t_{N-p-1}^{(1-0.025)}, \alpha^\mathrm{T}\hat{\beta}+\hat{\sigma}\sqrt{\alpha^\mathrm{T}(X^TX)^{-1}\alpha}\cdot t_{N-p-1}^{(1-0.025)}\right]
\]
for each testing point $\alpha$.
\item This corresponds to first calculating the confidence region $C_\beta$ then projecting $\beta\in C_\beta$ onto $\alpha$. The lower boundary of the confidence interval is determined by
\begin{align*}
&\min_\beta \alpha^\mathrm{T}\beta\\
\st& (\beta-\hat{\beta})^\mathrm{T}X^TX(\beta-\hat{\beta})\le\hat{\sigma}^2{\chi_{p+1}^2}^{(1-0.05)}
\end{align*}
Or equivalently,
\begin{align*}
&\min_z \alpha^\mathrm{T}z\\
\st& z^\mathrm{T}X^TXz\le\hat{\sigma}^2{\chi_{p+1}^2}^{(1-0.05)}
\end{align*}
By KKT conditions, this becomes
\begin{align*}
\frac{\partial}{\partial z}\left\{\alpha^{\mathrm{T}}z + \lambda\left[z^\mathrm{T}X^TXz-\hat{\sigma}^2{\chi_{p+1}^2}^{(1-0.05)}\right]\right\}\bigg|_{z=z^*}=& 0\\
\lambda\left[z^{*\mathrm{T}}X^TXz^*-\hat{\sigma}^2{\chi_{p+1}^2}^{(1-0.05)}\right] =& 0
\end{align*}
which gives
\begin{align*}
\lambda=&\frac{1}{2}\sqrt{\frac{\alpha^\mathrm{T}(X^TX)^{-1}\alpha}{\hat{\sigma}^2{\chi_{p+1}^2}^{1-0.95}}}\\
z^*=&\sqrt{\frac{\hat{\sigma}^2{\chi_{p+1}^2}^{1-0.95}}{\alpha^\mathrm{T}(X^TX)^{-1}\alpha}}\cdot(X^TX)^{-1}\alpha
\end{align*}
It can be easily verified that $\lambda\ge0$ and $z^\mathrm{*T}X^TXz^*\le\hat{\sigma}^2{\chi_{p+1}^2}^{(1-0.05)}$, so they are feasible. This gives the confidence band
\[
\left[\alpha^\mathrm{T}\hat{\beta}-\hat{\sigma}\sqrt{\alpha^\mathrm{T}(X^TX)^{-1}\alpha\cdot {\chi_{p+1}^2}^{(1-0.05)}},
\alpha^\mathrm{T}\hat{\beta}+\hat{\sigma}\sqrt{\alpha^\mathrm{T}(X^TX)^{-1}\alpha\cdot {\chi_{p+1}^2}^{(1-0.05)}}
\right]
\]
\end{enumerate}
For a fixed $p$, $\sqrt{{\chi_{p+1}^2}^{(1-0.05)}}-t_{N-p-1}^{(1-0.025)}$ grows monotonically when $N$ grows, with the limit as $\sqrt{{\chi_{p+1}^2}^{(1-0.05)}}-\Phi(1-0.025)$, where $\Phi$ is the probit function. For $p=3$ and $N\ge 5$, $\sqrt{{\chi_{p+1}^2}^{(1-0.05)}}>t_{N-p-1}^{(1-0.025)}$ from $N\ge 8$. The limit of the difference is $\sqrt{{\chi_{4}^2}^{(1-0.05)}}-\Phi(1-0.025)\approx 1.120252$.
\end{sol}

%3.3
\begin{sol}
\begin{enumerate}
\item
\label{3.3-1}
Let $\theta=a^\mathrm{T}\beta$, $\hat{\theta}=a^\mathrm{T}\hat{\beta}=b^\mathrm{T}y$ and $\tilde{\theta}=c^\mathrm{T}y$, where $\hat{\beta}=(X^{\mathrm{T}}X)^{-1}X^{\mathrm{T}}y$, $b=X(X^{\mathrm{T}}X)^{-1}a$ and $y\sim\mathcal{N}(X\beta,\sigma^2I)$. From the unbiasedness of $\tilde{\theta}$, we know $\E[\tilde{\theta}]=c^{\mathrm{T}}X\beta=\E[\hat{\theta}]=a^\mathrm{T}\beta$. Expanding $\var[\hat{\theta}]$ and $\var[\tilde{\theta}]$ we will have
\begin{align*}
\var[\hat{\theta}] =& b^\mathrm{T}(\sigma^2 I)b\\
=& \sigma^2 a^\mathrm{T}(X^{\mathrm{T}}X)^{-1}X^\mathrm{T}X(X^{\mathrm{T}}X)^{-1}a\\
=& \sigma^2 a^\mathrm{T}(X^{\mathrm{T}}X)^{-1}a\\
\var[\tilde{\theta}] =& c^\mathrm{T}(\sigma^2 I)c\\
=& \sigma^2 c^\mathrm{T}c
\end{align*}
Now consider
\begin{align*}
\min_c \ & c^\mathrm{T}c\\
\st & c^{\mathrm{T}}X\beta=a^\mathrm{T}\beta
\end{align*}
The Lagrangian $L=c^\mathrm{T}c+\lambda(c^{\mathrm{T}}X\beta-a^\mathrm{T}\beta)$ gives the stationary condition:
\begin{align*}
\frac{\partial L}{\partial c}=& 2c+\lambda X\beta=0\\
\frac{\partial L}{\partial \lambda}=& c^{\mathrm{T}}X\beta-a^\mathrm{T}\beta=0
\end{align*}
Canceling the $\lambda$ gives
\[
c=\frac{a^\mathrm{T}\beta}{\beta^{\mathrm{T}}X^{\mathrm{T}}X\beta}\cdot X\beta
\]
So
\begin{align*}
& c^\mathrm{T}c\ge \frac{\left(a^\mathrm{T}\beta\right)^2}{\beta^{\mathrm{T}}X^{\mathrm{T}}X\beta}=\frac{\beta^{\mathrm{T}}aa^{\mathrm{T}}\beta}{\beta^{\mathrm{T}}X^{\mathrm{T}}X\beta}\\
\Longrightarrow & c^\mathrm{T}c\ge \max_\beta \frac{\beta^{\mathrm{T}}aa^{\mathrm{T}}\beta}{\beta^{\mathrm{T}}X^{\mathrm{T}}X\beta}
\end{align*}
The final optimization problem is equivalent to
\begin{align*}
\max_\beta \ & \beta^{\mathrm{T}}aa^{\mathrm{T}}\beta\\
\st & \beta^{\mathrm{T}}X^{\mathrm{T}}X\beta=1
\end{align*}
Again by Lagrange multiplier, we can see that the maximum is the largest eigenvalue, denoted as $\mu$, of $A=aa^{\mathrm{T}}(X^{\mathrm{T}}X)^{-1}$. But since $\mathrm{rank}(A)=1$, so $\mu=\tr(A)=a^{\mathrm{T}}(X^{\mathrm{T}}X)^{-1}a$.
\item We similarly let $\tilde{\beta}=Cy\in\mathbb{R}^{p+1}$, then $\hat{V}=\sigma^2(X^\mathrm{T}X)^{-1}$, $\tilde{V}=\sigma^2CC^{\mathrm{T}}$ and $\E[\tilde{\beta}]=CX\beta=\beta$. From \ref{3.3-1}, $\forall a\in\mathbb{R}^{p+1}$, given $a^{\mathrm{T}}CX\beta=a^{\mathrm{T}}\beta$, we have
\begin{align*}
& a^{\mathrm{T}}CC^{\mathrm{T}}a\ge a^{\mathrm{T}}(X^\mathrm{T}X)^{-1}a\\
\Longrightarrow & a^{\mathrm{T}}\left(CC^{\mathrm{T}}-(X^\mathrm{T}X)^{-1}\right)a\ge 0\\
\Longrightarrow & CC^{\mathrm{T}} - \left(X^\mathrm{T}X\right)^{-1} \succeq 0\\
\Longrightarrow & CC^{\mathrm{T}}\succeq \left(X^\mathrm{T}X\right)^{-1}
\end{align*}
\end{enumerate}
\end{sol}

%3.4
\begin{sol}
From $X=QR$ one can get $\hat{\beta}=(X^\mathrm{T}X)^{-1}X^\mathrm{T}y=R^{-1}Q^\mathrm{T}y\Longrightarrow R\hat{\beta}=Q^\mathrm{T}y$. Further we denote
\begin{align*}
X =& (x_1,\cdots,x_{p+1})\\
Q =& (e_1,\cdots,e_{p+1})=(\frac{z_1}{\|z_1\|},\cdots,\frac{z_{p+1}}{\|z_{p+1}\|})\\
r_{ij} =& e_i^\mathrm{T}x_j
\end{align*}
Then we can write the equations involving individual $\hat{\beta}_i$ in a ``bottom-up'' way
\begin{align*}
(e_{p+1}^\mathrm{T}x_{p+1})\hat{\beta}_{p+1} &= e_{p+1}^\mathrm{T}y\\
(e_{p}^\mathrm{T}x_{p})\hat{\beta}_{p} + (e_{p}^\mathrm{T}x_{p+1})\hat{\beta}_{p+1} &= e_{p}^\mathrm{T}y\\
&\vdotswithin{=}\\
(e_{1}^\mathrm{T}x_{1})\hat{\beta}_{1} + \cdots + (e_{1}^\mathrm{T}x_{p+1})\hat{\beta}_{p+1} &= e_{1}^\mathrm{T}y
\end{align*}
This can be solved sequentially:
\begin{align*}
\hat{\beta}_{p+1} &= \frac{e_{p+1}^\mathrm{T}y}{e_{p+1}^\mathrm{T}x_{p+1}}\\
\hat{\beta}_{p} &= \frac{e_{p}^\mathrm{T}y}{e_{p}^\mathrm{T}x_{p}}-\frac{e_{p}^\mathrm{T}x_{p+1}}{e_{p}^\mathrm{T}x_{p}}\hat{\beta}_{p+1}\\
&\vdotswithin{=}\\
\hat{\beta}_{1} &= \frac{e_{1}^\mathrm{T}y}{e_{1}^\mathrm{T}x_{1}}- \frac{e_{1}^\mathrm{T}x_{2}}{e_{1}^\mathrm{T}x_{1}}\hat{\beta}_{2} - \cdots - \frac{e_{1}^\mathrm{T}x_{p+1}}{e_{1}^\mathrm{T}x_{1}}\hat{\beta}_{p+1}
\end{align*}
\end{sol}

%3.5
\begin{sol}
\begin{enumerate}
\item Ridge regression. From
\begin{align*}
\mathcal{L} =& \sum_{i=1}^{N}\left(y_i-\beta_0-\sum_{j=1}^{p}x_{ij}\beta_j\right)^2+\lambda\sum_{j=1}^{p}\beta_j^2\\
=& \sum_{i=1}^{N}\left[y_i-\left(\beta_0+\sum_{j=1}^{p}\bar{x}_j\beta_j\right)-\sum_{j=1}^{p}\left(x_{ij}-\bar{x}_j\right)\beta_j\right]^2+\lambda\sum_{j=1}^{p}\beta_j^2
\end{align*}
we can re-parameterize $\beta$ accordingly:
\[
\begin{cases}
\beta_0^c=\beta_0+\sum_{j=1}^{p}\bar{x}_j\beta_j\\
\beta_j^c=\beta_j\quad(1\le j\le p)
\end{cases}
\]
By setting $\partial \mathcal{L}/\partial \beta^c_0=0$, we have
\begin{align*}
& \sum_{i=1}^{N}\left[y_i-\beta_0^c-\sum_{j=1}^{p}\left(x_{ij}-\bar{x}_j\right)\beta^c_j\right]=0\\
\Longrightarrow & \beta_0^c=\frac{1}{N}\sum_{i=1}^{N}y_i-\frac{1}{N}\sum_{j=1}^{p}\sum_{i=1}^{N}\left(x_{ij}-\bar{x}_j\right)\beta^c_j=\frac{1}{N}\sum_{i=1}^{N}y_i
\end{align*}
Then one can follow the routine of ridge regression to get other $\beta_j^c$ with the centered design matrix.
\item Lasso. The regularization term does not affect the derivation of $\beta_0^c$, so we have the same $\beta_0^c$, with other $\beta_j^c$ obtained from any Lasso solver.
\end{enumerate}
\end{sol}

%3.6
\begin{sol}
From
\begin{align*}
P(\beta\vert y,X) \propto & P(y\vert\beta,X)P(\beta)\\
\propto& \exp\left[-\frac{1}{2\sigma^2}\left(\|y-X\beta\|^2+\frac{\sigma^2}{\tau}\|\beta\|^2\right)\right]
\end{align*}
we immediately know that $\lambda=\sigma^2/\tau$ and ridge regression estimate is the mode of the posterior distribution. To prove that it's also the mean, we let $\hat{\beta}$ be the ridge regression estimate:
\[
\hat{\beta}=\left(X^\mathrm{T}X+\frac{\sigma^2}{\tau}I\right)^{-1}X^\mathrm{T}y
\]
then
\begin{align*}
& \|y-X\beta\|^2+\frac{\sigma^2}{\tau}\|\beta\|^2 \\
=& \beta^\mathrm{T}\left(X^\mathrm{T}X+\frac{\sigma^2}{\tau}I\right)\beta - 2y^\mathrm{T}X\beta + y^\mathrm{T}y\\
=& (\beta-\hat{\beta})^\mathrm{T}\left(X^\mathrm{T}X+\frac{\sigma^2}{\tau}I\right)(\beta-\hat{\beta})-\hat{\beta}^\mathrm{T}\left(X^\mathrm{T}X+\frac{\sigma^2}{\tau}I\right)\hat{\beta}+\|y\|^2
\end{align*}
which can be minimized when $\beta=\hat{\beta}$. So $\hat{\beta}$ also maximizes the likelihood of the posterior distribution.
\end{sol}

%3.7
\begin{sol}
Similarly, the equivalence can be seen from
\begin{align*}
P(\beta_0,\beta\vert y,X) \propto & P(y\vert\beta_0,\beta,X)P(\beta)\\
=& \prod_i P(y_i\vert\beta_0,\beta,X)\cdot\prod_j P(\beta_j)\\
\propto& \prod_i \exp\left[-\frac{1}{2\sigma^2}\left(y_i-\beta_0-x_i^\mathrm{T}\beta\right)\right] \cdot \prod_j \exp\left(-\frac{\beta_j^2}{2\tau^2}\right)\\
=& \exp\left\{-\frac{1}{2\sigma^2}\left[\sum_i \left(y_i-\beta_0-\sum_jx_{ij}\beta_j\right)+\frac{\sigma^2}{\tau^2}\sum_j \beta_j^2\right]\right\}
\end{align*}
\end{sol}

%3.8
\begin{sol}
Let $X_2$ be the feature matrix without the column of 1's. We first consider the case where $N>p$ and $X_2$ is not singular. 
From
\[
X=(e,X_2)=QR=\left(\frac{1}{\sqrt{n}}e,Q_2\right)
\begin{pmatrix}
\sqrt{n} & \frac{1}{\sqrt{n}}e^\mathrm{T}X_2 \\
0 & R_2
\end{pmatrix}
=\left(e,\frac{1}{n}ee^\mathrm{T}X_2+Q_2R_2\right)
\]
we know that 
\[
\tilde{X}=\left(I-\frac{1}{n}ee^\mathrm{T}\right)X_2=Q_2R_2
\]
Meanwhile we have $\tilde{X}=U\Sigma V^\mathrm{T}$, which gives $U=Q_2R_2(\Sigma V^\mathrm{T})^{-1}$, implying $Q_2$ and $U$ span the same subspace. If $Q_2=U$ (up to sign flips, same below), then $R_2=U^\mathrm{T}U\Sigma V^\mathrm{T}=\Sigma V^\mathrm{T}$, that is, rows of $R_2$ are orthogonal. But $R_2$ is upper triangle, so $R_2$ is diagonal, and subsequently the columns of $\tilde{X}$ are orthogonal.

In the case where $N\le p$ or $X$ is singular, $R_2$ and $V$ are not square matrices any more. One needs to replace the inverse with pseudo-inverse to get $\mathrm{span}(Q_2)=\mathrm{span}(U)$. But the condition for $Q_2=U$ is more difficult to find.
\end{sol}

%3.9
\begin{sol}
Let $X_1=QR$ and $Q=(z_1,\cdots,z_q)$. Suppose a new $\tilde{x}$ comes from the columns of $X_2$, we have
\[
(X_1, \tilde{x})=\tilde{Q}\tilde{R}=\left(Q,\frac{\tilde{x}-QQ^\mathrm{T}\tilde{x}}{\|\tilde{x}-QQ^\mathrm{T}\tilde{x}\|}\right)
\begin{pmatrix}
R & Q^\mathrm{T}\tilde{x}\\
0 & \|\tilde{x}-QQ^\mathrm{T}\tilde{x}\|
\end{pmatrix}
\]
Denote $z_{q+1}=(\tilde{x}-QQ^\mathrm{T}\tilde{x})/\|\tilde{x}-QQ^\mathrm{T}\tilde{x}\|$, $\Omega=\mathrm{span}(Q)$, and $\tilde{\Omega}=\mathrm{span}(Q,z_{q+1})=\mathrm{span}(Q,\tilde{x})$, then it's easy to verify
$\Omega\subset\tilde{\Omega}$, $QQ^\mathrm{T}y\in \Omega$, $\tilde{Q}\tilde{Q}^\mathrm{T}y\in \tilde{\Omega}$, $y-QQ^\mathrm{T}y\in \Omega^\perp$, and $y-\tilde{Q}\tilde{Q}^\mathrm{T}y\in \tilde{\Omega}^\perp$. Define $\tilde{r}$ as the new residue, we have
\begin{align*}
r =& \|y-QQ^\mathrm{T}y\|^2\\
=& \|(y-\tilde{Q}\tilde{Q}^\mathrm{T}y)+(\tilde{Q}\tilde{Q}^\mathrm{T}y-QQ^\mathrm{T}y)\|^2\\
=& \tilde{r} + \|\tilde{Q}\tilde{Q}^\mathrm{T}y-QQ^\mathrm{T}y\|^2 + 2(y-\tilde{Q}\tilde{Q}^\mathrm{T}y)^\mathrm{T}(\tilde{Q}\tilde{Q}^\mathrm{T}y-QQ^\mathrm{T}y)\\
=& \tilde{r} + \|(\tilde{Q}\tilde{Q}^\mathrm{T}-QQ^\mathrm{T})y\|^2\\
=& \tilde{r} + \left\|\left(\frac{\tilde{x}-QQ^\mathrm{T}\tilde{x}}{\|\tilde{x}-QQ^\mathrm{T}\tilde{x}\|}\right)\left(\frac{\tilde{x}-QQ^\mathrm{T}\tilde{x}}{\|\tilde{x}-QQ^\mathrm{T}\tilde{x}\|}\right)^\mathrm{T}y\right\|^2\\
=& \tilde{r} + \left[\tilde{x}^\mathrm{T}(I-QQ^\mathrm{T})y\right]^2
\end{align*}
So $\tilde{r}=r-[\tilde{x}^\mathrm{T}(I-QQ^\mathrm{T})y]^2$. In practice, one can use this update rule to find the $\tilde{x}$ that gives the largest $\left[\tilde{x}^\mathrm{T}(I-QQ^\mathrm{T})y\right]^2$.
\end{sol}

%3.10
\begin{sol}
As discussed in \thesection.\ref{3.1}, just alternatingly drop the variable with the smallest Z-score and refit the model.
\end{sol}

%3.11
\begin{sol}
If $\Sigma_i=\Sigma$,
\begin{align*}
& \frac{\partial}{\partial B}\sum_{i=1}^N\left(y_i-B^\mathrm{T}x_i\right)^\mathrm{T}\Sigma^{-1}\left(y_i-B^\mathrm{T}x_i\right)=0\\
\Longrightarrow & \sum_{i=1}^N\Sigma^{-1}\left(y_i-B^\mathrm{T}x_i\right)x_i^\mathrm{T}=0\\
\Longrightarrow & \Sigma^{-1}(Y^\mathrm{T}-B^\mathrm{T}X^\mathrm{T})X=0\\
\Longrightarrow & B=(X^\mathrm{T}X)^{-1}X^\mathrm{T}Y
\end{align*}
If $\Sigma_i$ are different, let $\vect(\cdot)$ be the operator that ``column-ordered'' a matrix into a vector, then
\begin{align*}
& \sum_{i=1}^N\Sigma_i^{-1}\left(y_i-B^\mathrm{T}x_i\right)x_i^\mathrm{T}=0\\
\Longrightarrow & \sum_{i=1}^{N}x_ix_i^\mathrm{T}B\Sigma_i^{-1}=\sum_{i=1}^{N}x_iy_i^\mathrm{T}\Sigma_i^{-1} \\
\Longrightarrow & \left(\sum_{i=1}^{N}\Sigma_i^{-1}\otimes x_ix_i^\mathrm{T}\right)\cdot\vect(B)=\vect\left(\sum_{i=1}^{N}x_iy_i^\mathrm{T}\Sigma_i^{-1}\right)\\
\Longrightarrow & \vect(B)=\left(\sum_{i=1}^{N}\Sigma_i^{-1}\otimes x_ix_i^\mathrm{T}\right)^{-1}\cdot\vect\left(\sum_{i=1}^{N}x_iy_i^\mathrm{T}\Sigma_i^{-1}\right)
\end{align*}
In the last equality, if we set $\Sigma_i=\Sigma$, it becomes
\begin{align*}
\vect(B) =& \left(\Sigma^{-1}\otimes\sum_{i=1}^{N}x_ix_i^\mathrm{T}\right)^{-1} \left(\Sigma^{-1}\otimes I\right)\cdot\vect\left(\sum_{i=1}^{N}x_iy_i^\mathrm{T}\right)\\
=& \left[\Sigma\otimes\left(\sum_{i=1}^{N}x_ix_i^\mathrm{T}\right)^{-1}\right] \left(\Sigma^{-1}\otimes I\right)\cdot\vect\left(\sum_{i=1}^{N}x_iy_i^\mathrm{T}\right)\\
=& \left[\Sigma\otimes\left(X^\mathrm{T}X\right)^{-1}\right] \left(\Sigma^{-1}\otimes I\right)\cdot\vect\left(X^\mathrm{T}Y\right)\\
=& I\otimes \left(X^\mathrm{T}X\right)^{-1}\cdot \vect\left(X^\mathrm{T}Y\right)\\
=& \vect\left[\left(X^\mathrm{T}X\right)^{-1}X^\mathrm{T}Y\right]
\end{align*}
which again comes to
\[
B=\left(X^\mathrm{T}X\right)^{-1}X^\mathrm{T}Y
\]
\end{sol}